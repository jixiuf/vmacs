#   -*- coding: utf-8 -*-
# Rime dictionary: qq五笔导出
# cat w.txt|sort -r| awk -F\t '!a[$1$2]++'|sort>s2.txt
#  awk -F\t '!a[$1$2]++' 意思是以tab分隔，第一列第二列作key,用来去重，

# 差集
# https://www.jianshu.com/p/cf428138caab
# 意思是以tab分隔，第一列作key,只打印key在b.txt中存在，且不存在于a.txt中的
# awk -F\t 'NR==FNR{ a[$1]=$1 } NR>FNR{ if(a[$1] == ""){ print $0}}' a.txt b.txt
#NR表示已经处理的行数，FNR表示当前文件处理的行数。所以当NR==FNR时，处理的是aaa.txt，NR>FNR时处理的是bbb.txt
# 即在处理a.txt时 将a[key]设置上值，在处理b.txt时，判断a[key]是否为空，为空就打印，即为空，说明此key在a.txt中不存在
# awk   -F\t 'NR==FNR{ a[$1]=$1 } NR>FNR{ if(a[$1] == ""){ print $0}}' wubi_pinyin_jixiuf.base.dict.yaml wubi_pinyin_jixiuf.suyu.dict.yaml>c.txt

# awk -F\t 'NR==FNR{ a[$1]=$1 } NR>FNR{ if(a[$1] == ""){ print $1}}' a.txt b.txt

# a.txt-b.txt:
# sort a.txt b.txt b.txt | uniq -u
# sort a.txt b.txt b.txt | uniq -u：将两个文件排序，最后输出a.txt b.txt b.txt文件中只出现过一次的内容，
# 因为有两个b.txt所以只会输出只在a.txt出现过一次的内容，即是a.txt-b.txt差集

# 求交集 sort a.txt b.txt | uniq -d


# Rime dictionary ---

#  egret_wubi.extended 这个词典的神奇之处是：虽然
# egret_wubi.schema.yaml 已设置为加载名为 egret_wubi.extended 的词典，
# 但配套的用户词典名却是「egret_wubi」，即 Rime 自动取句点之前的部分为
# 用户词典名，以保证按以上方法增补了词汇之后，不至於因为改变词典名而抛弃
# 原有的用户词典。
---

name: egret_wubi
# name: egret_wubi.extended
version: "2013.04.22"
sort: by_weight
# use_preset_vocabulary: true
import_tables:
  # 这里引入别的字库dict
  - dicts/wubi/wubi.base
  - dicts/wubi/wubi
  - dicts/wubi/wubi.word4
  # - dicts/wubi/wubi.suyu
  # - cn_dicts_common/tencent_core # 精简的腾讯词库
  # - cn_dicts_common/tencent  # 腾讯词向量（大词库，部署时间较长 选择性开启）
  # - cn_dicts_common/user # 用户自定义词典
columns:
  - text
  - code
  - weight
  - stem
encoder:
  exclude_patterns:
    - '^z.*$'
  rules:
    - length_equal: 2           # #對於二字詞
      formula: "AaAbBaBb"       # #取第一字首碼与第二码、第二字首碼及第二码
    - length_equal: 3           #
      formula: "AaBaCaCb"       # 取第1字首码，第2字首码，第3字首码及第2码
    - length_in_range: [4, 10]
      formula: "AaBaCaZa"

...
# 以下写用户自定义的词
# table begins
# 以下为我自己加的词，每次加新词要加wubi_pinyin_jixiuf.userdb/目录清掉重新部署
# columns: 定義碼表以Tab分隔出的各列，可設text【文本】、code【碼】、weight【權重】、stem【造詞碼】
# tab 隔开
不孬	gigi	1

